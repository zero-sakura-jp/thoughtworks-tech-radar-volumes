name,ring,quadrant,isNew,status,description
Application,Retire,techniques,FALSE,moved in,"<p>For many years, we've used the <a href=""https://martinfowler.com/bliki/CanaryRelease.html"" target=""_blank"" aria-label=""canary release. This is an external link. Opens in new tab"" class=""pop-out"">canary release<span class=""pop-out-icon""></span></a> approach to encourage early feedback on new software versions, while reducing risk through incremental rollout to selected users. The <strong>1% canary</strong> is a useful technique where we roll out new features to a very small segment (say 1%) of users carefully chosen across various user categories. This enables teams to capture fast user feedback, observe the impact of new releases on performance and stability and respond as necessary. This technique becomes especially crucial when teams are rolling out software updates to mobile applications or a fleet of devices like edge computing devices or software-defined vehicles. With proper observability and early feedback, it gives the opportunity to contain the blast radius in the event of unexpected scenarios in production. While canary releases can be useful to get faster user feedback, we believe starting with a small percentage of users is mandatory to reduce and contain the risk of large-scale feature rollouts.</p>"
Using GenAI to understand legacy codebases,Contain,techniques,FALSE,moved in,"<p>Generative AI (GenAI) and large language models (LLMs) can help developers write and understand code. Help with understanding code is especially useful in the case of legacy codebases with poor, out-of-date or misleading documentation. Since we last wrote about this, techniques and products for <strong>using GenAI to understand legacy codebases</strong> have further evolved, and we've successfully used some of them in practice, notably to <a href=""https://martinfowler.com/articles/legacy-modernization-gen-ai.html"" target=""_blank"" aria-label=""assist reverse engineering efforts for mainframe modernization. This is an external link. Opens in new tab"" class=""pop-out"">assist reverse engineering efforts for mainframe modernization<span class=""pop-out-icon""></span></a>. A particularly promising technique we've used is a <a href=""https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag"">retrieval-augmented generation (RAG)</a> approach where the information retrieval is done on a knowledge graph of the codebase. The knowledge graph can preserve structural information about the codebase beyond what an LLM could derive from the textual code alone. This is particularly helpful in legacy codebases that are less self-descriptive and cohesive. An additional opportunity to improve code understanding is that the graph can be further enriched with existing and AI-generated documentation, external dependencies, business domain knowledge or whatever else is available that can make the AI's job easier.</p>"
AI team assistants,Baseline,techniques,FALSE,no change,"<p>AI coding assistance tools are mostly talked about in the context of assisting and enhancing an individual contributor's work. However, software delivery is and will remain team work, so you should be looking for ways to create <strong>AI team assistants</strong> that help create the 10x team, as opposed to a bunch of siloed AI-assisted <a href=""https://www.thoughtworks.com/radar/techniques/10x-engineers"">10x engineers</a>. Fortunately, recent developments in the tools market are moving us closer to making this a reality. <a href=""https://www.thoughtworks.com/radar/platforms/unblocked"">Unblocked</a> is a platform that pulls together all of a team's knowledge sources and integrates them intelligently into team members' tools. And <a href=""https://www.atlassian.com/software/rovo"" target=""_blank"" aria-label=""Atlassian's Rovo. This is an external link. Opens in new tab"" class=""pop-out"">Atlassian's Rovo<span class=""pop-out-icon""></span></a> brings AI into the most widely used team collaboration platform, giving teams new types of search and access to their documentation, in addition to unlocking new ways of automation and software practice support with Rovo agents. While we wait for the market to further evolve in this space, we've been exploring the potential of AI for knowledge amplification and team practice support ourselves: We open-sourced our <a href=""https://github.com/tw-haiven/haiven"" target=""_blank"" aria-label=""Haiven team assistant. This is an external link. Opens in new tab"" class=""pop-out"">Haiven team assistant<span class=""pop-out-icon""></span></a> and started gathering learnings with <a href=""https://www.thoughtworks.com/insights/blog/generative-ai/using-ai-requirements-analysis-case-study"">AI assistance for noncoding tasks like requirements analysis</a>.</p>"
Structured output from LLMs,Target,techniques,TRUE,new,"<p><strong>Structured output from LLMs</strong> refers to the practice of constraining a language model's response into a defined schema. This can be achieved either through instructing a generalized model to respond in a particular format or by fine-tuning a model so it ""natively"" outputs, for example, JSON. OpenAI now supports structured output, allowing developers to supply a JSON Schema, <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/pydantic"">pydantic</a> or Zod object to constrain model responses. This capability is particularly valuable for enabling function calling, API interactions and external integrations, where accuracy and adherence to a format are critical. Structured output not only enhances the way LLMs can interface with code but also supports broader use cases like generating markup for rendering charts. Additionally, structured output has been shown to reduce the chance of hallucinations within model output.</p>"
Replacing pair programming with AI,Retire,techniques,TRUE,new,"<p>When people talk about coding assistants, the topic of <a href=""https://martinfowler.com/articles/on-pair-programming.html"" target=""_blank"" aria-label=""pair programming. This is an external link. Opens in new tab"" class=""pop-out"">pair programming<span class=""pop-out-icon""></span></a> inevitably comes up. Our profession has a love-hate relationship with it: some swear by it, others can't stand it. Coding assistants now beg the question, can a human pair with the AI, instead of another human, and get the same results for the team? <a href=""https://www.thoughtworks.com/radar/tools/github-copilot"">GitHub Copilot</a> even calls itself ""your AI pair programmer."" While we do think a coding assistant can bring some of the benefits of pair programming, we advise against fully <strong><a href=""https://martinfowler.com/articles/exploring-gen-ai.html#memo-05"" target=""_blank"" aria-label=""replacing pair programming with AI. This is an external link. Opens in new tab"" class=""pop-out"">replacing pair programming with AI<span class=""pop-out-icon""></span></a></strong>. Framing coding assistants as pair programmers ignores one of the key benefits of pairing: to make the team, not just the individual contributors, better. Coding assistants can offer benefits for getting unstuck, learning about a new technology, onboarding or making tactical work faster so that we can focus on the strategic design. But they don't help with any of the team collaboration benefits, like keeping the work-in-progress low, reducing handoffs and relearning, making continuous integration possible or improving collective code ownership.</p>"
Vespa,Contain,platforms,TRUE,new,"<p><strong><a href=""https://vespa.ai/"" target=""_blank"" aria-label=""Vespa. This is an external link. Opens in new tab"" class=""pop-out"">Vespa<span class=""pop-out-icon""></span></a></strong> is an open-source search engine and big data processing platform. It's particularly well-suited for applications that require low latency and high throughput. Our teams like Vespa's ability to implement hybrid search using multiple retrieval techniques, to efficiently filter and sort many types of metadata, to implement multi-phased ranking, to index multiple vectors (e.g., for each chunk) per document without duplicating all the metadata into separately indexed documents and to retrieve data from multiple indexed fields at once.</p>"
Unblocked,Baseline,platforms,TRUE,new,"<p><strong><a href=""https://getunblocked.com"" target=""_blank"" aria-label=""Unblocked. This is an external link. Opens in new tab"" class=""pop-out"">Unblocked<span class=""pop-out-icon""></span></a></strong> provides software development lifecycle (SDLC) asset and artifact discovery. It integrates with common application lifecycle management (ALM) and collaboration tools to help teams understand codebases and related resources. It improves code comprehension by delivering immediate, relevant context about the code, making it easier to navigate and understand complex systems. Engineering teams can securely and compliantly access discussions, assets and documents related to their work. Unblocked also captures and shares local knowledge that often resides with experienced team members, making valuable insights accessible to everyone, regardless of experience level.</p>"
Wiz,Retire,tools,FALSE,moved in,"<p><strong><a href=""https://www.wiz.io/"" target=""_blank"" aria-label=""Wiz. This is an external link. Opens in new tab"" class=""pop-out"">Wiz<span class=""pop-out-icon""></span></a></strong> has emerged as the cloud security platform of choice on many of our projects. Our teams appreciate that it enables them to detect risks and threats sooner than similar tools as it continuously scans for changes. Wiz can detect and alert on misconfigurations, vulnerabilities and leaked secrets both in artifacts that have yet to be deployed to live environments (container images, infrastructure code) as well as live workloads (containers, VMs and cloud services). We also appreciate the powerful reporting capability for both development teams and leadership. This analysis helps us understand how a vulnerability can affect a given service so that we can resolve issues in that context.</p>"
Unleash,Contain,tools,TRUE,new,"<p>Although using the <a href=""https://www.thoughtworks.com/radar/techniques/simplest-possible-feature-toggle"">simplest feature toggle possible</a> remains our recommended approach, scaling teams and faster development make managing hand-crafted toggles more complex. <strong><a href=""https://www.getunleash.io/"" target=""_blank"" aria-label=""Unleash. This is an external link. Opens in new tab"" class=""pop-out"">Unleash<span class=""pop-out-icon""></span></a></strong> is an option widely used by our teams to address this complexity and enable CI/CD. It can be used either as a service or self-hosted. It provides SDKs in several languages with a good developer experience and friendly UI for administration. Although there’s no official support for the <a href=""https://openfeature.dev/"" target=""_blank"" aria-label=""OpenFeature specification. This is an external link. Opens in new tab"" class=""pop-out"">OpenFeature specification<span class=""pop-out-icon""></span></a> yet, you can find community-maintained <a href=""https://openfeature.dev/ecosystem?instant_search%5BrefinementList%5D%5Bvendor%5D%5B0%5D=Unleash"" target=""_blank"" aria-label=""providers. This is an external link. Opens in new tab"" class=""pop-out"">providers<span class=""pop-out-icon""></span></a> for Go and Java. Unleash can be used for simple feature toggles as well as segmentation and gradual rollouts, making it a suitable option for feature management at scale.</p>"
Zed,Baseline,tools,TRUE,new,"<p>After the shutdown of the <a href=""https://en.wikipedia.org/wiki/Atom_(text_editor)#History"" target=""_blank"" aria-label=""Atom. This is an external link. Opens in new tab"" class=""pop-out"">Atom<span class=""pop-out-icon""></span></a> text editor project, its creators built a new editor named <strong><a href=""https://zed.dev/"" target=""_blank"" aria-label=""Zed. This is an external link. Opens in new tab"" class=""pop-out"">Zed<span class=""pop-out-icon""></span></a></strong>. Written in Rust and optimized to leverage modern hardware, Zed feels fast. It has all the features we expect from a modern editor: support for many programming languages, a built-in terminal and multibuffer editing to name a few. AI-assisted coding is available through integration with several LLM providers. As ardent pair programmers we're intrigued by the remote <a href=""https://zed.dev/docs/collaboration"" target=""_blank"" aria-label=""collaboration feature. This is an external link. Opens in new tab"" class=""pop-out"">collaboration feature<span class=""pop-out-icon""></span></a> built into Zed. Developers find each other via their GitHub IDs and can then collaborate on the same workspace in real time. It's too early to tell whether development teams can and want to escape the pull of the <a href=""https://www.thoughtworks.com/radar/tools/visual-studio-code"">Visual Studio Code</a> ecosystem, but Zed is an alternative to explore.</p>"
CocoaPods,T,tools,TRUE,new,"<p><strong><a href=""https://cocoapods.org/"" target=""_blank"" aria-label=""CocoaPods. This is an external link. Opens in new tab"" class=""pop-out"">CocoaPods<span class=""pop-out-icon""></span></a></strong> has been a popular dependency management tool for Swift and Objective-C Cocoa projects. However, the CocoaPods team <a href=""https://blog.cocoapods.org/CocoaPods-Support-Plans/"" target=""_blank"" aria-label=""announced. This is an external link. Opens in new tab"" class=""pop-out"">announced<span class=""pop-out-icon""></span></a> that the project is in maintenance mode after more than a decade of being a key tool for iOS and macOS developers. While the tool and its resources will remain available, active development will cease. Developers are encouraged to transition to <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/swift-package-manager"">Swift Package Manager</a>, which offers native integration with Xcode and better long-term support from Apple.</p>"
Testcontainers,Retire,languages-and-frameworks,FALSE,no change,"<p>In our experience, <strong><a href=""https://www.testcontainers.org/"" target=""_blank"" aria-label=""Testcontainers. This is an external link. Opens in new tab"" class=""pop-out"">Testcontainers<span class=""pop-out-icon""></span></a></strong> are a useful default option for creating a reliable environment for running tests. It's a library, ported to multiple languages, that Dockerizes common test dependencies — including various types of databases, queuing technologies, cloud services and UI testing dependencies like web browsers — with the ability to run custom Dockerfiles when needed. Recently, a <a href=""https://testcontainers.com/desktop/"" target=""_blank"" aria-label=""desktop version. This is an external link. Opens in new tab"" class=""pop-out"">desktop version<span class=""pop-out-icon""></span></a> was released that allows for the visual management of test sessions and the ability to manage more complex scenarios which our teams have found very useful.</p>"
vLLM,Contain,languages-and-frameworks,FALSE,moved in,"<p><strong><a href=""https://github.com/vllm-project/vllm"" target=""_blank"" aria-label=""vLLM. This is an external link. Opens in new tab"" class=""pop-out"">vLLM<span class=""pop-out-icon""></span></a></strong> is a high-throughput, memory-efficient inference engine for LLMs that can run in the cloud or on-premise. It seamlessly supports multiple <a href=""https://docs.vllm.ai/en/latest/models/supported_models.html"" target=""_blank"" aria-label=""model architectures. This is an external link. Opens in new tab"" class=""pop-out"">model architectures<span class=""pop-out-icon""></span></a> and popular open-source models. Our teams deploy dockerized vLLM workers on GPU platforms like NVIDIA DGX and Intel HPC, hosting models such as <a href=""https://huggingface.co/blog/llama31"" target=""_blank"" aria-label=""Llama 3.1(8B and 70B). This is an external link. Opens in new tab"" class=""pop-out"">Llama 3.1(8B and 70B)<span class=""pop-out-icon""></span></a>, <a href=""https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3"" target=""_blank"" aria-label=""Mistral 7B. This is an external link. Opens in new tab"" class=""pop-out"">Mistral 7B<span class=""pop-out-icon""></span></a> and <a href=""https://huggingface.co/defog/llama-3-sqlcoder-8b"" target=""_blank"" aria-label=""Llama-SQL. This is an external link. Opens in new tab"" class=""pop-out"">Llama-SQL<span class=""pop-out-icon""></span></a> for developer coding assistance, knowledge search and natural language database interactions. vLLM is compatible with the OpenAI SDK standard, facilitating consistent model serving. Azure's <a href=""https://ai.azure.com/explore/models"" target=""_blank"" aria-label=""AI Model Catalog. This is an external link. Opens in new tab"" class=""pop-out"">AI Model Catalog<span class=""pop-out-icon""></span></a> uses a custom inference container to enhance model serving performance, with vLLM as the default inference engine due to its high throughput and efficient memory management. The vLLM framework is emerging as a default for large-scale model deployments.</p>"
SST,Baseline,languages-and-frameworks,TRUE,new,"<p><strong><a href=""https://sst.dev/"" target=""_blank"" aria-label=""SST. This is an external link. Opens in new tab"" class=""pop-out"">SST<span class=""pop-out-icon""></span></a></strong> is a framework for deploying applications into a cloud environment along with provisioning all the services that the application needs to run. SST is not just an <a href=""https://www.thoughtworks.com/radar/techniques/infrastructure-as-code"">IaC</a> tool; it's a framework with a TypeScript API that enables you to define your application environment, a service that deploys your application when triggered on a Git push as well as a GUI console to manage the resulting application and invoke the SST management features. Although SST was originally based on AWS Cloud Formation and CDK, its latest version has been implemented on top of <a href=""https://www.thoughtworks.com/radar/tools/terraform"">Terraform</a> and <a href=""https://www.thoughtworks.com/radar/platforms/pulumi"">Pulumi</a> so that, in theory, it's cloud agnostic. SST has native support for deploying several standard web application frameworks, including <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/next-js"">Next.js</a> and <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/remix"">Remix</a>, but also supports headless API applications. SST appears to be in a category of its own. While it bears some resemblance to <a href=""https://www.thoughtworks.com/radar/techniques/platform-orchestration"">platform orchestration</a> tools like Kubevela, it also provides developer conveniences like a live mode that proxies AWS Lambda invocations back to a function running on the developer's local machine. Right now, SST remains a bit of a curiosity, but it is a project and part of a category of tools worth watching as it evolves.</p>"
